{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wandb \n",
    "import random\n",
    "\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_from_img(img_path: Path): \n",
    "    return \"-\".join(str(img_path.stem).split(\".\")[0].split(\"-\")[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CLIP / model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathologyPairDataset(data.Dataset): \n",
    "    def __init__(\n",
    "        self, \n",
    "        img_dir: Path, \n",
    "        txt_dir: Path, \n",
    "        case_ids: List[int], \n",
    "        transform = None, \n",
    "    ): \n",
    "        \"\"\"\n",
    "        img_dir and txt_dir. \n",
    "\n",
    "        Use case_ids to perform train_test split at the start,\n",
    "        can choose which cases are actually read in\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.txt_dir = txt_dir\n",
    "        self.case_ids = case_ids\n",
    "        self.transform = transform\n",
    "\n",
    "        # len determined by number of image paths\n",
    "        self.img_paths = list(img_dir.rglob(\"*/*.png\"))\n",
    "        self.txt_paths = list(txt_dir.rglob(\"*.txt\"))\n",
    "        \n",
    "        self.case2txt = defaultdict(str)\n",
    "        for txt_path in self.txt_paths: \n",
    "            case_id = str(txt_path.stem).split(\".\")[0]\n",
    "            if case_id in case_ids:\n",
    "                self.case2txt[case_id] = txt_path\n",
    "        \n",
    "        # amortize case lookup\n",
    "        self.img_txt_pairs = []\n",
    "        for img_path in self.img_paths: \n",
    "            case_id = \"-\".join(str(img_path.stem).split(\".\")[0].split(\"-\")[:3])\n",
    "            if case_id in case_ids:\n",
    "                self.img_txt_pairs.append((img_path, self.case2txt[case_id]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_txt_pairs)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_path, txt_path = self.img_txt_pairs[idx]\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        with open(txt_path, \"r\") as f: \n",
    "            txt = f.read()\n",
    "\n",
    "        return img, txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get valid cases, match images with texts\n",
    "img_dir = Path(\"../../../../data/data/patches/10.0_224\")\n",
    "txt_dir = Path(\"../../../../data/data/reports\")\n",
    "\n",
    "patch_folders = list(img_dir.glob(\"*\"))\n",
    "txt_paths = list(txt_dir.rglob(\"*.txt\"))\n",
    "\n",
    "# get unique case ids from img_paths\n",
    "unique_case_ids = [extract_case_from_img(patch_folder) for patch_folder in patch_folders]\n",
    "assert(len(np.unique(unique_case_ids)) == len(unique_case_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of dataset:  1999969\n"
     ]
    }
   ],
   "source": [
    "# read in from csv, sample one from each class\n",
    "labels_path = Path(\"../../../../data/data\") / \"labels_40.csv\"\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "all_cases = list(labels_df[\"Case ID\"])\n",
    "\n",
    "dataset = PathologyPairDataset(\n",
    "    img_dir=img_dir, \n",
    "    txt_dir=txt_dir,\n",
    "    case_ids=unique_case_ids,\n",
    "    transform=preprocess, \n",
    ")\n",
    "print(\"Len of dataset: \", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "batch_size: int = 64\n",
    "shuffle: int = True\n",
    "num_workers: int = 4\n",
    "\n",
    "loader_params = {\n",
    "    'batch_size': batch_size, \n",
    "    'shuffle': shuffle, \n",
    "    'num_workers': num_workers, \n",
    "}\n",
    "data_loader = data.DataLoader(dataset, **loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts, model):\n",
    "    _tokenizer = SimpleTokenizer()\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), model.context_length, dtype=torch.long)\n",
    "    \n",
    "    # will trim tokens to match context length\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > model.context_length:\n",
    "            tokens = tokens[:model.context_length]\n",
    "            tokens[model.context_length - 1] = eot_token\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mekintiu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/projects/CS224n_Final_Project/wandb/run-20230309_061820-a9wgdutd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ekintiu/path-clip-v0/runs/a9wgdutd' target=\"_blank\">rose-dawn-1</a></strong> to <a href='https://wandb.ai/ekintiu/path-clip-v0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ekintiu/path-clip-v0' target=\"_blank\">https://wandb.ai/ekintiu/path-clip-v0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ekintiu/path-clip-v0/runs/a9wgdutd' target=\"_blank\">https://wandb.ai/ekintiu/path-clip-v0/runs/a9wgdutd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ekintiu/path-clip-v0/runs/a9wgdutd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7facae35ba30>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr: float = 1e-4\n",
    "momentum: float = 0.9\n",
    "epochs: int = 4\n",
    "log_interval: int = 100\n",
    "save_interval: int = 1000\n",
    "save_dir: Path = Path(\"./checkpoints\")\n",
    "model_name: str = \"sample_0\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# init wandb config\n",
    "config = {\n",
    "    \"lr\": lr,\n",
    "    \"momentum\": momentum, \n",
    "    \"epochs\": epochs, \n",
    "    \"log_interval\": log_interval, \n",
    "    \"save_interval\": save_interval, \n",
    "    \"save_dir\": \"save_dir\", \n",
    "    \"model_name\": \"sample_0\", \n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"path-clip-v0\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, device, criterion, optimizer, config): \n",
    "    model_save_dir = save_dir / model_name\n",
    "    model_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Run training\n",
    "    total_batches = len(loader) * epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    report_freq = log_interval\n",
    "    highest_val_auc = 0 # save highest mean auc\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0 # running loss over batch\n",
    "        for data in tqdm(loader):\n",
    "            # get the images\n",
    "            images, texts = data\n",
    "            texts = preprocess_text(texts, model) \n",
    "            \n",
    "            # perform step for a single batch\n",
    "            loss = train_batch(images, texts, model, device, criterion, optimizer)\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Report metrics every `report_freq` batch\n",
    "            if (batch_ct % report_freq) == 0:\n",
    "                train_log(running_loss / report_freq, example_ct, epoch)\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            if (batch_ct % save_interval) == 0: \n",
    "                model_path = model_save_dir / f\"checkpoint_{str(batch_ct)}.pt\"\n",
    "                print(\"Saved checkpoint to: \", model_path)\n",
    "                save(model, model_path)\n",
    "                \n",
    "def train_batch(images, texts, model, device, criterion, optimizer):\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "    \n",
    "    # Forward pass ➡\n",
    "    logits_per_image, logits_per_text = model(images, texts)\n",
    "    \n",
    "    # Create labels\n",
    "    batch_size = images.shape[0]\n",
    "    labels = torch.arange(batch_size).to(device)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_img = criterion(logits_per_image, labels)\n",
    "    loss_txt = criterion(logits_per_text, labels)\n",
    "    loss = (loss_img + loss_txt)/2 # avg. img and txt loss\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # save to wandb \n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "    # print to log\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "def save(model, path): \n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/31250 [01:03<5:29:16,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 06400 examples: 3.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 200/31250 [02:04<5:20:54,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 12800 examples: 2.362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 300/31250 [03:05<5:18:08,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 19200 examples: 1.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 400/31250 [04:07<5:44:51,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 25600 examples: 1.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 500/31250 [05:09<5:07:01,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 32000 examples: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 600/31250 [06:12<5:03:42,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 38400 examples: 0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 700/31250 [07:12<5:09:27,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 44800 examples: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 800/31250 [08:13<5:09:09,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 51200 examples: 0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 831/31250 [08:32<5:15:06,  1.61it/s]"
     ]
    }
   ],
   "source": [
    "train(model, data_loader, device, criterion, optimizer, config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
