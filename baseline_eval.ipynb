{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "import clip\n",
    "from pathlib import Path\n",
    "\n",
    "# load in clip\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Optional\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/__init__.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clip vit models -- this should have pre-trained clip weights\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in from csv, sample one from each class\n",
    "labels_path = data_dir / \"labels_40.csv\"\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "# add column to labels \n",
    "labels_df[\"label\"] = labels_df.apply(lambda x: x['Label'].split(\" \")[0].lower(),axis=1)\n",
    "\n",
    "sample_label_cases = list(labels_df.groupby(\"Label\").first()['Case ID'])\n",
    "labels_sample = labels_df[labels_df['Case ID'].isin(sample_label_cases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-CJ-4642</td>\n",
       "      <td>Adenomas and Adenocarcinomas</td>\n",
       "      <td>adenomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TCGA-BA-4077</td>\n",
       "      <td>Squamous Cell Neoplasms</td>\n",
       "      <td>squamous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TCGA-A8-A084</td>\n",
       "      <td>Ductal and Lobular Neoplasms</td>\n",
       "      <td>ductal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TCGA-06-0209</td>\n",
       "      <td>Gliomas</td>\n",
       "      <td>gliomas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Case ID                         Label     label\n",
       "0   TCGA-CJ-4642  Adenomas and Adenocarcinomas  adenomas\n",
       "10  TCGA-BA-4077       Squamous Cell Neoplasms  squamous\n",
       "20  TCGA-A8-A084  Ductal and Lobular Neoplasms    ductal\n",
       "30  TCGA-06-0209                       Gliomas   gliomas"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all labels \n",
    "all_cases = list(labels_df[\"Case ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svs_paths(all_svs: List[Path], cases: List[str]):\n",
    "    # Returns single svs for each patient given a set of cases and a list of all svs\n",
    "    svs_counts = defaultdict(int)\n",
    "    svs_paths = []\n",
    "    for svs_path in all_svs: \n",
    "        for sample_case in cases: \n",
    "            if sample_case in str(svs_path): \n",
    "                if svs_counts[sample_case] == 0: \n",
    "                    svs_paths.append(svs_path)\n",
    "                svs_counts[sample_case] += 1\n",
    "    return svs_paths\n",
    "\n",
    "all_svs = list(data_dir.rglob(\"./*.svs\"))\n",
    "svs_paths = get_svs_paths(all_svs=all_svs, cases=sample_label_cases)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/TCGA-CJ-4642/TCGA-CJ-4642-01B-01-BS1.5a1225ca-0cb1-4be4-852d-701b1b1a4e67.svs'),\n",
       " PosixPath('data/TCGA-BA-4077/TCGA-BA-4077-01B-01-TS1.f5a59d10-d032-4d7a-9244-71c31954b122.svs'),\n",
       " PosixPath('data/TCGA-A8-A084/TCGA-A8-A084-01Z-00-DX1.2B52D1B8-5AD4-4BD6-ADF7-9D65B8EE2621.svs'),\n",
       " PosixPath('data/TCGA-06-0209/TCGA-06-0209-01Z-00-DX8.4a540299-b778-43e4-b80b-f70d5f222378.svs')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svs_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parse some 224x224 patches from some slides\n",
    "# THIS WAS NOT USED\n",
    "def extract_patches(paths: List[Path], output_dir: Path, resolution: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Extract 224x224 patches from SVS files that are not masked out by an Otsu threshold.\n",
    "\n",
    "    :param paths: list of paths to SVS files\n",
    "    :param output_dir: output directory to save patches\n",
    "    :param resolution: resolution level to extract patches from\n",
    "    \"\"\"\n",
    "    # Create the output directory if it does not exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        # Load the SVS file with OpenSlide\n",
    "        slide = openslide.open_slide(str(path))\n",
    "\n",
    "        if resolution is None: \n",
    "            resolution = slide.level_count - 1\n",
    "\n",
    "        # Compute the dimensions of the slide at the given resolution level\n",
    "        w, h = slide.level_dimensions[resolution]\n",
    "\n",
    "        # Compute the dimensions of a 224x224 patch at the given resolution level\n",
    "        patch_size = 224\n",
    "        pw = (w - patch_size) // patch_size + 1\n",
    "        ph = (h - patch_size) // patch_size + 1\n",
    "\n",
    "        # Iterate over the patches and extract those that are not masked out by an Otsu threshold\n",
    "        for x in range(pw):\n",
    "            for y in range(ph):\n",
    "                # Compute the coordinates of the patch in the slide\n",
    "                x0 = x * patch_size\n",
    "                y0 = y * patch_size\n",
    "                x1 = x0 + patch_size\n",
    "                y1 = y0 + patch_size\n",
    "\n",
    "                # Read the region from the slide\n",
    "                region = slide.read_region((x0, y0), resolution, (patch_size, patch_size))\n",
    "\n",
    "                # Convert the region to a NumPy array and extract the grayscale channel\n",
    "                img = np.array(region.convert('L'))\n",
    "\n",
    "                # Compute the Otsu threshold and binarize the image\n",
    "                threshold, mask = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "                # If the mask is all white, the patch is not masked out\n",
    "                # if np.all(mask == 255):\n",
    "                # Save the patch to the output directory with the desired file name\n",
    "                patch_name = f\"{path.stem}_{x}_{y}.png\"\n",
    "                patch_path = Path(output_dir) / patch_name\n",
    "                cv2.imwrite(str(patch_path), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# output_dir = data_dir / \"patches\"\n",
    "# extract_patches(paths=svs_paths, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dir = data_dir / \"patches\"\n",
    "patch_paths = list(patch_dir.rglob(\"./*.png\"))\n",
    "\n",
    "sample_image = preprocess(Image.open(patch_paths[0])).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    image_features = model.encode_image(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on full test dataset\n",
    "test_data_dir = patch_dir\n",
    "test_dataset = datasets.ImageFolder(test_data_dir, transform=preprocess)\n",
    "# Create a DataLoader to load the test dataset in batches\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_list = []\n",
    "labels_list = []\n",
    "for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        features = model.encode_image(images)\n",
    "    logits_list.append(features)\n",
    "    labels_list.append(labels)\n",
    "logits = torch.cat(logits_list, dim=0)\n",
    "labels = torch.cat(labels_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=4, bias=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataLoader to load the test dataset in batches\n",
    "num_ftrs = 512\n",
    "num_classes = 4\n",
    "topk = 5\n",
    "\n",
    "# Define the linear classifier and the loss function\n",
    "linear_classifier = nn.Linear(num_ftrs, num_classes)\n",
    "linear_classifier = linear_classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for the linear classifier\n",
    "optimizer = optim.SGD(linear_classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Set the linear classifier to training mode\n",
    "linear_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 641.9414\n",
      "Epoch [2/30], Loss: 3942.0457\n",
      "Epoch [3/30], Loss: 3229.3350\n",
      "Epoch [4/30], Loss: 1966.2836\n",
      "Epoch [5/30], Loss: 1020.1500\n",
      "Epoch [6/30], Loss: 1009.6738\n",
      "Epoch [7/30], Loss: 1297.4641\n",
      "Epoch [8/30], Loss: 873.2217\n",
      "Epoch [9/30], Loss: 503.1060\n",
      "Epoch [10/30], Loss: 421.4751\n",
      "Epoch [11/30], Loss: 339.7691\n",
      "Epoch [12/30], Loss: 375.5216\n",
      "Epoch [13/30], Loss: 301.1740\n",
      "Epoch [14/30], Loss: 205.1505\n",
      "Epoch [15/30], Loss: 241.1382\n",
      "Epoch [16/30], Loss: 274.3821\n",
      "Epoch [17/30], Loss: 231.8254\n",
      "Epoch [18/30], Loss: 231.4314\n",
      "Epoch [19/30], Loss: 160.9533\n",
      "Epoch [20/30], Loss: 117.9594\n",
      "Epoch [21/30], Loss: 95.8594\n",
      "Epoch [22/30], Loss: 137.1429\n",
      "Epoch [23/30], Loss: 98.7828\n",
      "Epoch [24/30], Loss: 124.5679\n",
      "Epoch [25/30], Loss: 114.5041\n",
      "Epoch [26/30], Loss: 97.3875\n",
      "Epoch [27/30], Loss: 81.7098\n",
      "Epoch [28/30], Loss: 45.9641\n",
      "Epoch [29/30], Loss: 61.1156\n",
      "Epoch [30/30], Loss: 49.0063\n"
     ]
    }
   ],
   "source": [
    "# Train the linear classifier on the pre-trained logits using the test dataset\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # train for each of the logits and labels, want to batch this if possible\n",
    "    for i, (features, targets) in enumerate(zip(logits, labels)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = linear_classifier(features.float())\n",
    "        loss = criterion(outputs.unsqueeze(0), targets.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the model predictions for the test dataset\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model.encode_image(images) # clip \n",
    "        output = linear_classifier(output.float())\n",
    "        \n",
    "        y_pred.append(output)\n",
    "        y_true.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume y_pred is a tensor of shape (num_examples, num_classes) containing the outputs of the linear classifier\n",
    "# and y_true is a tensor of shape (num_examples) containing the categorical labels\n",
    "y_pred = torch.cat(y_pred, dim=0)\n",
    "y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "y_prob = torch.softmax(y_pred, dim=1)\n",
    "y_true = y_true.detach().cpu().numpy()\n",
    "y_prob = y_prob.detach().cpu().numpy()\n",
    "# apply softmax to the outputs of the linear classifier to obtain the predicted probabilities\n",
    "\n",
    "# compute the AUROC for each class using the predicted probabilities and the corresponding categorical labels\n",
    "aurocs = []\n",
    "for c in range(num_classes):\n",
    "    y_true_c = (y_true == c)\n",
    "    y_prob_c = y_prob[:, c]\n",
    "    auroc_c = roc_auc_score(y_true_c, y_prob_c)\n",
    "    aurocs.append(auroc_c)\n",
    "\n",
    "# compute the mean AUROC over all classes\n",
    "mean_auroc = sum(aurocs) / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiclass AUROC:  [0.9942294662831576, 0.9977590617273394, 0.9804071770480807, 0.9525653519752824] 0.981240264258465\n"
     ]
    }
   ],
   "source": [
    "print(\"multiclass AUROC: \", aurocs, mean_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc@k, prec@k, rec@k\n",
    "def top_k_acc_prec_rec(y_prob, y_true, k):\n",
    "    # y_prob is a numpy array of shape (num_examples, num_classes) containing the predicted probabilities\n",
    "    # y_true is a numpy array of shape (num_examples) containing the true labels\n",
    "    # k is the number of classes to consider for top-k accuracy, precision, and recall\n",
    "\n",
    "    # get the indices of the top k predicted probabilities for each example\n",
    "    top_k_preds = np.argsort(y_prob, axis=1)[:, -k:]\n",
    "\n",
    "    # compute accuracy@k, precision@k, and recall@k for each example\n",
    "    acc_k = np.mean(np.any(top_k_preds == y_true[:, np.newaxis], axis=1))\n",
    "    prec_k = np.mean([np.any(top_k_preds[i] == y_true[i]) for i in range(len(y_true))])\n",
    "    rec_k = np.mean([np.any(y_true[i] == top_k_preds[i]) for i in range(len(y_true))])\n",
    "\n",
    "    return acc_k, prec_k, rec_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_k, prec_k, rec_k = top_k_acc_prec_rec(y_prob, y_true, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc@k:  0.7682047584715213\n",
      "prec@k:  0.7682047584715213\n",
      "rec@k:  0.7682047584715213\n"
     ]
    }
   ],
   "source": [
    "print(\"acc@k: \", acc_k)\n",
    "print(\"prec@k: \", prec_k)\n",
    "print(\"rec@k: \", rec_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981240264258465\n"
     ]
    }
   ],
   "source": [
    "print(mean_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 4\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43movr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# compute the accuracy@k, precision@k, and recall@k for each example in the test dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:560\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=557'>558</a>\u001b[0m     \u001b[39mif\u001b[39;00m multi_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=558'>559</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulti_class must be in (\u001b[39m\u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=559'>560</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=560'>561</a>\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=561'>562</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=562'>563</a>\u001b[0m \u001b[39melif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=563'>564</a>\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:627\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=624'>625</a>\u001b[0m \u001b[39m# validation of the input y_score\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=625'>626</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mallclose(\u001b[39m1\u001b[39m, y_score\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)):\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=626'>627</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=627'>628</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTarget scores need to be probabilities for multiclass \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=628'>629</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroc_auc, i.e. they should sum up to 1.0 over classes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=629'>630</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=631'>632</a>\u001b[0m \u001b[39m# validation for multiclass parameter specifications\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_ranking.py?line=632'>633</a>\u001b[0m average_options \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes"
     ]
    }
   ],
   "source": [
    "# convert the predictions to numpy arrays and compute the AUC\n",
    "y_pred = y_pred.detach().cpu().numpy()\n",
    "y_true = y_true.detach().cpu().numpy()\n",
    "auc = roc_auc_score(y_true, y_pred, multi_class='ovr')\n",
    "\n",
    "# compute the accuracy@k, precision@k, and recall@k for each example in the test dataset\n",
    "num_examples = len(y_true)\n",
    "acc_topk = np.zeros(num_examples)\n",
    "prec_topk = np.zeros(num_examples)\n",
    "rec_topk = np.zeros(num_examples)\n",
    "for i in range(num_examples):\n",
    "    y_true_i = y_true[i]\n",
    "    y_pred_i = y_pred[i]\n",
    "    topk_preds = np.argsort(y_pred_i)[-topk:]\n",
    "    acc_topk[i] = y_true_i in topk_preds\n",
    "    prec_topk[i] = precision_score(y_true_i, topk_preds, average='micro')\n",
    "    rec_topk[i] = recall_score(y_true_i, topk_preds, average='micro')\n",
    "\n",
    "# compute the average metrics over the entire test set\n",
    "acc_topk_mean = np.mean(acc_topk)\n",
    "prec_topk_mean = np.mean(prec_topk)\n",
    "rec_topk_mean = np.mean(rec_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the linear classifier on the test dataset\n",
    "linear_classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "topk = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        features = model.encode_image(images)\n",
    "        outputs = linear_classifier(features.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.589041095890411\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', correct / total)\n",
    "# TODO: test all the necessary evaluation metrics -- i.e. top k stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchLoader(data.Dataset):\n",
    "    def __init__(self, svs_folder, label_df_path, otsu_threshold=0.5, patch_size=224):\n",
    "        self.svs_folder = svs_folder\n",
    "        self.label_df = pd.read_csv(label_df_path)\n",
    "        self.otsu_threshold = otsu_threshold\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Generate list of valid patch coordinates and corresponding labels\n",
    "        self.valid_coords, self.labels = self.generate_valid_coords()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get patch coordinates\n",
    "        svs_file, x, y = self.valid_coords[idx]\n",
    "\n",
    "        # Read patch from slide\n",
    "        slide = openslide.OpenSlide(os.path.join(self.svs_folder, svs_file))\n",
    "        patch = slide.read_region((x, y), 0, (self.patch_size, self.patch_size)).convert(\"RGB\")\n",
    "        patch = np.asarray(patch)[:, :, :3]  # Remove alpha channel if present\n",
    "\n",
    "        # Apply Otsu threshold\n",
    "        gray_patch = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n",
    "        _, mask = cv2.threshold(gray_patch, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        mask = mask / 255  # Normalize to [0, 1]\n",
    "        if np.mean(mask) >= self.otsu_threshold:\n",
    "            # If patch is too dark, generate a new patch\n",
    "            return self.__getitem__(np.random.randint(len(self)))\n",
    "\n",
    "        # Apply random augmentations if desired\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(90),\n",
    "        ])\n",
    "        patch = transform(patch)\n",
    "\n",
    "        # Convert patch to tensor\n",
    "        patch = transforms.ToTensor()(patch)\n",
    "\n",
    "        # Get corresponding label\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return patch, label\n",
    "\n",
    "    def generate_valid_coords(self):\n",
    "        valid_coords = []\n",
    "        labels = []\n",
    "        for svs_file in os.listdir(self.svs_folder):\n",
    "            if not svs_file.endswith('.svs'):\n",
    "                continue\n",
    "            label = self.label_df.loc[svs_file[:-4]]['label']\n",
    "            slide = openslide.OpenSlide(os.path.join(self.svs_folder, svs_file))\n",
    "            for y in range(self.patch_size // 2, slide.dimensions[1] - self.patch_size // 2, self.patch_size):\n",
    "                for x in range(self.patch_size // 2, slide.dimensions[0] - self.patch_size // 2, self.patch_size):\n",
    "                    # Read patch from slide\n",
    "                    patch = slide.read_region((x, y), 0, (self.patch_size, self.patch_size)).convert(\"RGB\")\n",
    "                    patch = np.asarray(patch)[:, :, :3]  # Remove alpha channel if present\n",
    "\n",
    "                    # Apply Otsu threshold\n",
    "                    gray_patch = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n",
    "                    _, mask = cv2.threshold(gray_patch, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "                    mask = mask / 255  # Normalize to [0, 1]\n",
    "                    if np.mean(mask) < self.otsu_threshold:\n",
    "                        valid_coords.append((svs_file, x, y))\n",
    "                        labels.append(label)\n",
    "        return valid_coords, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "svs_folder = \"../data/sample/svs/\"\n",
    "label_df_path = \"../data/sample/labels.csv\"\n",
    "\n",
    "patch_dataset = PatchLoader(\n",
    "    svs_folder=svs_folder, \n",
    "    label_df_path=label_df_path,\n",
    ")\n",
    "\n",
    "# Define the batch size and other DataLoader options\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "shuffle = False\n",
    "\n",
    "# Create the data loader\n",
    "my_dataloader = DataLoader(\n",
    "    dataset=patch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=shuffle,\n",
    "    # Additional DataLoader options go here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate(model, data_loader, k=5):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables for metrics\n",
    "    total_images = 0\n",
    "    total_top_k_correct = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the data loader\n",
    "        for images, labels in data_loader:\n",
    "            # Send images and labels to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get predictions from model\n",
    "            outputs = model(images)\n",
    "            # TODO: go from model logits --> predictions\n",
    "            \n",
    "            preds = torch.topk(outputs, k=k, dim=1)[1]\n",
    "\n",
    "            # Compute metrics\n",
    "            total_images += len(labels)\n",
    "            total_top_k_correct += (preds == labels.view(-1,1)).sum().item()\n",
    "\n",
    "            precision, recall = top_k_precision_recall(preds, labels.view(-1,1), k=k)\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "    # Compute multiclass AUROC\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    auroc = roc_auc_score(y_true=all_labels, y_score=all_preds, multi_class='ovo')\n",
    "\n",
    "    # Compute top-k accuracy, precision, and recall\n",
    "    top_k_accuracy = total_top_k_correct / total_images\n",
    "    top_k_precision = total_precision / len(data_loader)\n",
    "    top_k_recall = total_recall / len(data_loader)\n",
    "\n",
    "    return top_k_accuracy, auroc, top_k_precision, top_k_recall\n",
    "\n",
    "def top_k_precision_recall(preds, labels, k=5):\n",
    "    # Compute precision and recall for top-k predictions\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(preds)):\n",
    "        pred = preds[i][:k]\n",
    "        label = labels[i]\n",
    "        if label in pred:\n",
    "            tp += 1\n",
    "            fp += k - 1\n",
    "            fn += 0\n",
    "        else:\n",
    "            tp += 0\n",
    "            fp += k\n",
    "            fn += 1\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    return precision, recall\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
