{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import clip\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# load in clip\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Optional\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clip vit models -- this should have pre-trained clip weights\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in from csv, sample one from each class\n",
    "labels_path = data_dir / \"labels_40.csv\"\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "# add column to labels \n",
    "labels_df[\"label\"] = labels_df.apply(lambda x: x['Label'].split(\" \")[0].lower(),axis=1)\n",
    "\n",
    "sample_label_cases = list(labels_df.groupby(\"Label\").first()['Case ID'])\n",
    "labels_sample = labels_df[labels_df['Case ID'].isin(sample_label_cases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-CJ-4642</td>\n",
       "      <td>Adenomas and Adenocarcinomas</td>\n",
       "      <td>adenomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TCGA-BA-4077</td>\n",
       "      <td>Squamous Cell Neoplasms</td>\n",
       "      <td>squamous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TCGA-A8-A084</td>\n",
       "      <td>Ductal and Lobular Neoplasms</td>\n",
       "      <td>ductal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TCGA-06-0209</td>\n",
       "      <td>Gliomas</td>\n",
       "      <td>gliomas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Case ID                         Label     label\n",
       "0   TCGA-CJ-4642  Adenomas and Adenocarcinomas  adenomas\n",
       "10  TCGA-BA-4077       Squamous Cell Neoplasms  squamous\n",
       "20  TCGA-A8-A084  Ductal and Lobular Neoplasms    ductal\n",
       "30  TCGA-06-0209                       Gliomas   gliomas"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all labels \n",
    "all_cases = list(labels_df[\"Case ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svs_paths(all_svs: List[Path], cases: List[str]):\n",
    "    # Returns single svs for each patient given a set of cases and a list of all svs\n",
    "    svs_counts = defaultdict(int)\n",
    "    svs_paths = []\n",
    "    for svs_path in all_svs: # iterate through each svs\n",
    "        for sample_case in cases: # for each case\n",
    "            if sample_case in str(svs_path) and \"DX\" in str(svs_path): \n",
    "                if svs_counts[sample_case] == 0: \n",
    "                    svs_paths.append(svs_path)\n",
    "                svs_counts[sample_case] += 1\n",
    "    return svs_paths\n",
    "\n",
    "all_svs = list(data_dir.rglob(\"./*.svs\"))\n",
    "svs_paths = get_svs_paths(all_svs=all_svs, cases=all_cases)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svs_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make copies in separate folder\n",
    "# slides_dir = data_dir / \"slides\"\n",
    "# for svs in tqdm(svs_paths): \n",
    "#     shutil.copy(svs, slides_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move patches into correct folders\n",
    "# do train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parse some 224x224 patches from some slides\n",
    "# THIS WAS NOT USED\n",
    "def extract_patches(paths: List[Path], output_dir: Path, resolution: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Extract 224x224 patches from SVS files that are not masked out by an Otsu threshold.\n",
    "\n",
    "    :param paths: list of paths to SVS files\n",
    "    :param output_dir: output directory to save patches\n",
    "    :param resolution: resolution level to extract patches from\n",
    "    \"\"\"\n",
    "    # Create the output directory if it does not exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        # Load the SVS file with OpenSlide\n",
    "        slide = openslide.open_slide(str(path))\n",
    "\n",
    "        if resolution is None: \n",
    "            resolution = slide.level_count - 1\n",
    "\n",
    "        # Compute the dimensions of the slide at the given resolution level\n",
    "        w, h = slide.level_dimensions[resolution]\n",
    "\n",
    "        # Compute the dimensions of a 224x224 patch at the given resolution level\n",
    "        patch_size = 224\n",
    "        pw = (w - patch_size) // patch_size + 1\n",
    "        ph = (h - patch_size) // patch_size + 1\n",
    "\n",
    "        # Iterate over the patches and extract those that are not masked out by an Otsu threshold\n",
    "        for x in range(pw):\n",
    "            for y in range(ph):\n",
    "                # Compute the coordinates of the patch in the slide\n",
    "                x0 = x * patch_size\n",
    "                y0 = y * patch_size\n",
    "                x1 = x0 + patch_size\n",
    "                y1 = y0 + patch_size\n",
    "\n",
    "                # Read the region from the slide\n",
    "                region = slide.read_region((x0, y0), resolution, (patch_size, patch_size))\n",
    "\n",
    "                # Convert the region to a NumPy array and extract the grayscale channel\n",
    "                img = np.array(region.convert('L'))\n",
    "\n",
    "                # Compute the Otsu threshold and binarize the image\n",
    "                threshold, mask = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "                # If the mask is all white, the patch is not masked out\n",
    "                # if np.all(mask == 255):\n",
    "                # Save the patch to the output directory with the desired file name\n",
    "                patch_name = f\"{path.stem}_{x}_{y}.png\"\n",
    "                patch_path = Path(output_dir) / patch_name\n",
    "                cv2.imwrite(str(patch_path), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = data_dir / \"patches\"\n",
    "# extract_patches(paths=svs_paths, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(data.Dataset):\n",
    "    def __init__(self, patch_paths: List[Path] , labels_df: pd.DataFrame, transform = None):\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            assumes `label_df` has columns `Case ID` and `label` mapping\n",
    "            TCGA id to corresponding class. \n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df\n",
    "        self.patch_paths = patch_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.last_idx = 0\n",
    "        self.labels_to_idx = dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get patch and preprocess\n",
    "        patch_path = self.patch_paths[idx]\n",
    "        patch_stem = patch_path.stem\n",
    "        case_id = \"-\".join(str(patch_stem).split(\".\")[0].split(\"-\")[:3])\n",
    "        patch = Image.open(patch_path)\n",
    "        if self.transform is not None:\n",
    "            patch = self.transform(patch)\n",
    "        \n",
    "        # Get corresponding label\n",
    "        label = self.labels_df[self.labels_df[\"Case ID\"] == case_id][\"label\"]\n",
    "        if len(label) == 0: \n",
    "            raise ValueError(\"Can't find corresponding label a given case.\")\n",
    "        label = label.values[0] # extract single label\n",
    "        \n",
    "#         print(label)\n",
    "        if label not in self.labels_to_idx: \n",
    "            self.labels_to_idx[label] = self.last_idx\n",
    "            self.last_idx += 1\n",
    "\n",
    "        label_idx = self.labels_to_idx[label]\n",
    "            \n",
    "        return patch, label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paths:  76252\n"
     ]
    }
   ],
   "source": [
    "patch_dir = data_dir / \"patches\" / \"10.0_224\"\n",
    "patch_paths = list(patch_dir.rglob(\"./*.png\"))\n",
    "print(\"Number of paths: \", len(patch_paths))\n",
    "# sample_image = preprocess(Image.open(patch_paths[0])).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_ids = np.unique([\"-\".join(str(patch_path.stem).split(\".\")[0].split(\"-\")[:3]) for patch_path in patch_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(unq_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patches containing each id\n",
    "def get_patches_from_ids(ids, patch_paths): \n",
    "    paths = []\n",
    "    for id in ids: \n",
    "        for patch in patch_paths: \n",
    "            if id in str(patch): \n",
    "                paths.append(patch)\n",
    "    return paths\n",
    "\n",
    "train_paths = get_patches_from_ids(ids=train_ids, patch_paths=patch_paths)\n",
    "test_paths = get_patches_from_ids(ids=test_ids, patch_paths=patch_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train paths:  60164\n"
     ]
    }
   ],
   "source": [
    "print(\"Num train paths: \", len(train_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num test paths:  16088\n"
     ]
    }
   ],
   "source": [
    "print(\"Num test paths: \", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad(): \n",
    "#     image_features = model.encode_image(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patches containing these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on full test dataset\n",
    "test_data_dir = patch_dir\n",
    "# test_dataset = datasets.ImageFolder(test_data_dir, transform=preprocess)\n",
    "train_dataset = PatchDataset(\n",
    "    patch_paths=train_paths,\n",
    "    labels_df=labels_df, \n",
    "    transform=preprocess,\n",
    ")\n",
    "\n",
    "test_dataset = PatchDataset(\n",
    "    patch_paths=test_paths, \n",
    "    labels_df=labels_df, \n",
    "    transform=preprocess,\n",
    ")\n",
    "\n",
    "# Create a DataLoader to load the test dataset in batches\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Evaluation\n",
    "* First, train a linear model on a train dataset\n",
    "* Second, evaluate linear model on a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1881/1881 [01:01<00:00, 30.76it/s]\n"
     ]
    }
   ],
   "source": [
    "logits_list = []\n",
    "labels_list = []\n",
    "for batch_idx, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        features = model.encode_image(images)\n",
    "        # move to gpu\n",
    "        features.to(device)\n",
    "        labels.to(device)\n",
    "        \n",
    "    logits_list.append(features)\n",
    "    labels_list.append(labels)\n",
    "logits = torch.cat(logits_list, dim=0)\n",
    "labels = torch.cat(labels_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=4, bias=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataLoader to load the test dataset in batches\n",
    "num_ftrs = 512\n",
    "num_classes = 4\n",
    "\n",
    "# Define the linear classifier and the loss function\n",
    "linear_classifier = nn.Linear(num_ftrs, num_classes)\n",
    "linear_classifier = linear_classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for the linear classifier\n",
    "optimizer = optim.SGD(linear_classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Set the linear classifier to training mode\n",
    "linear_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▊                                                                                                              | 1/30 [00:27<13:22, 27.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 21104.3469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███████▌                                                                                                          | 2/30 [00:55<12:55, 27.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Loss: 16809.4639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████▍                                                                                                      | 3/30 [01:22<12:23, 27.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Loss: 13853.9231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████████▏                                                                                                  | 4/30 [01:50<11:57, 27.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Loss: 13155.4045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████████                                                                                               | 5/30 [02:18<11:37, 27.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 13232.0555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████▊                                                                                           | 6/30 [02:46<11:10, 27.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Loss: 11967.5044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████████████▌                                                                                       | 7/30 [03:14<10:40, 27.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Loss: 11474.8261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████████████▍                                                                                   | 8/30 [03:42<10:10, 27.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Loss: 10956.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████▏                                                                               | 9/30 [04:09<09:41, 27.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Loss: 10453.4697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████████████████████████████▋                                                                           | 10/30 [04:37<09:14, 27.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 10026.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████████████████▋                                                                           | 10/30 [04:48<09:37, 28.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# train for each of the logits and labels, want to batch this if possible\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (features, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(logits, labels)):\n\u001b[0;32m----> 7\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzero_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, set_to_none: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m            the step altogether).\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_zero_grad_profile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_for_profile()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the linear classifier on the pre-trained logits using the test dataset\n",
    "num_epochs = 30\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    # train for each of the logits and labels, want to batch this if possible\n",
    "    for i, (features, targets) in enumerate(zip(logits, labels)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = linear_classifier(features.float())\n",
    "        loss = criterion(outputs.unsqueeze(0), targets.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/1881 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m linear_classifier(features\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels_one_hot)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# logits_list = []\n",
    "# labels_list = []\n",
    "\n",
    "# TODO: move into a function\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            features = model.encode_image(images)\n",
    "            features.to(device)\n",
    "            labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=4).float().to(device) # convert labels to one-hot encoding\n",
    "            outputs = linear_classifier(features.float())\n",
    "            loss = criterion(outputs, labels_one_hot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss))\n",
    "\n",
    "# logits = torch.cat(logits_list, dim=0)\n",
    "# labels = torch.cat(labels_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 503/503 [00:16<00:00, 31.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute the model predictions for the test dataset\n",
    "y_pred = []\n",
    "y_true = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model.encode_image(images) # clip \n",
    "        output = linear_classifier(output.float())\n",
    "        \n",
    "        y_pred.append(output)\n",
    "        y_true.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume y_pred is a tensor of shape (num_examples, num_classes) containing the outputs of the linear classifier\n",
    "# and y_true is a tensor of shape (num_examples) containing the categorical labels\n",
    "y_pred = torch.cat(y_pred, dim=0)\n",
    "y_true = torch.cat(y_true, dim=0)\n",
    "\n",
    "y_prob = torch.softmax(y_pred, dim=1)\n",
    "y_true = y_true.detach().cpu().numpy()\n",
    "y_prob = y_prob.detach().cpu().numpy()\n",
    "# apply softmax to the outputs of the linear classifier to obtain the predicted probabilities\n",
    "\n",
    "# compute the AUROC for each class using the predicted probabilities and the corresponding categorical labels\n",
    "aurocs = []\n",
    "for c in range(num_classes):\n",
    "    y_true_c = (y_true == c)\n",
    "    y_prob_c = y_prob[:, c]\n",
    "    auroc_c = roc_auc_score(y_true_c, y_prob_c)\n",
    "    aurocs.append(auroc_c)\n",
    "\n",
    "# compute the mean AUROC over all classes\n",
    "mean_auroc = sum(aurocs) / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiclass AUROC:  [0.9173501362357952, 0.6423820917906181, 0.85675162672, 0.35392481043158375] 0.6926021662944992\n"
     ]
    }
   ],
   "source": [
    "print(\"multiclass AUROC: \", aurocs, mean_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc@k, prec@k, rec@k\n",
    "def top_k_acc_prec_rec(y_prob, y_true, k):\n",
    "    # y_prob is a numpy array of shape (num_examples, num_classes) containing the predicted probabilities\n",
    "    # y_true is a numpy array of shape (num_examples) containing the true labels\n",
    "    # k is the number of classes to consider for top-k accuracy, precision, and recall\n",
    "\n",
    "    # get the indices of the top k predicted probabilities for each example\n",
    "    top_k_preds = np.argsort(y_prob, axis=1)[:, -k:]\n",
    "\n",
    "    # compute accuracy@k, precision@k, and recall@k for each example\n",
    "    acc_k = np.mean(np.any(top_k_preds == y_true[:, np.newaxis], axis=1))\n",
    "    prec_k = np.mean([np.any(top_k_preds[i] == y_true[i]) for i in range(len(y_true))])\n",
    "    rec_k = np.mean([np.any(y_true[i] == top_k_preds[i]) for i in range(len(y_true))])\n",
    "\n",
    "    return acc_k, prec_k, rec_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_k, prec_k, rec_k = top_k_acc_prec_rec(y_prob, y_true, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc@k:  0.4175783192441571\n",
      "prec@k:  0.4175783192441571\n",
      "rec@k:  0.4175783192441571\n"
     ]
    }
   ],
   "source": [
    "print(\"acc@k: \", acc_k)\n",
    "print(\"prec@k: \", prec_k)\n",
    "print(\"rec@k: \", rec_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6926021662944992\n"
     ]
    }
   ],
   "source": [
    "print(mean_auroc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
