{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wandb \n",
    "import random\n",
    "\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "from loader import PathologyPairDataset\n",
    "# from builder import PathZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_from_img(img_path: Path): \n",
    "    return \"-\".join(str(img_path.stem).split(\".\")[0].split(\"-\")[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get valid cases, match images with texts\n",
    "img_dir = Path(\"../../../../data/data/patches/pt\")\n",
    "txt_dir = Path(\"../../../../data/data/reports\")\n",
    "\n",
    "patch_folders = list(img_dir.glob(\"*.pt\"))\n",
    "txt_paths = list(txt_dir.rglob(\"*.txt\"))\n",
    "\n",
    "# get unique case ids from img_paths\n",
    "unique_case_ids = [extract_case_from_img(patch_folder) for patch_folder in patch_folders]\n",
    "assert(len(np.unique(unique_case_ids)) == len(unique_case_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of dataset:  1332\n"
     ]
    }
   ],
   "source": [
    "# read in from csv, sample one from each class\n",
    "labels_path = Path(\"../../../../data/data\") / \"labels_40.csv\"\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "all_cases = list(labels_df[\"Case ID\"])\n",
    "\n",
    "dataset = PathologyPairDataset(\n",
    "    img_dir=img_dir, \n",
    "    txt_dir=txt_dir,\n",
    "    case_ids=unique_case_ids,\n",
    "    transform=None, \n",
    ")\n",
    "print(\"Len of dataset: \", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "batch_size: int = 8\n",
    "shuffle: int = False\n",
    "num_workers: int = 0\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Get the maximum sequence length in the batch for the first output\n",
    "    max_len = max([x[0].shape[0] for x in batch])\n",
    "\n",
    "    # Pad all sequences in the batch to the same length for the first output\n",
    "    padded_batch_0 = []\n",
    "    for x in batch:\n",
    "        padded_seq = torch.zeros((max_len, 2048), dtype=torch.float)\n",
    "        padded_seq[:x[0].shape[0], :] = x[0]\n",
    "        padded_batch_0.append(padded_seq)\n",
    "\n",
    "    # Stack the padded sequences into a tensor for the first output\n",
    "    padded_batch_0 = torch.stack(padded_batch_0)\n",
    "\n",
    "    # Stack the second and third outputs into tensors\n",
    "    padded_batch_1 = [x[1] for x in batch]\n",
    "    padded_batch_2 = [x[2] for x in batch]\n",
    "\n",
    "    return padded_batch_0, padded_batch_1, padded_batch_2\n",
    "\n",
    "loader_params = {\n",
    "    'batch_size': batch_size, \n",
    "    'shuffle': shuffle, \n",
    "    'num_workers': num_workers, \n",
    "    'collate_fn': collate_fn, \n",
    "}\n",
    "\n",
    "data_loader = data.DataLoader(dataset, **loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load CLIP / model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from os.path import join\n",
    "import pdb\n",
    "from turtle import forward\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "\n",
    "###########################\n",
    "### PathZero Implementation ###\n",
    "###########################\n",
    "class PathZero(nn.Module): \n",
    "    def __init__(\n",
    "        self, \n",
    "        image_input_dim: int = 2048, # d_i \n",
    "        embedding_dim: int = 256, # d_e\n",
    "        logits_dim: int = 256, # d_l\n",
    "        dropout: float = 0.25,\n",
    "        clip_model: Optional[nn.Module] = None\n",
    "    ): \n",
    "        super(PathZero, self).__init__()\n",
    "\n",
    "        self.image_input_dim = image_input_dim \n",
    "        self.embedding_dim = embedding_dim # dim for coattn\n",
    "        self.logits_dim = logits_dim # dim for contrastive\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        ## FC over WSI bag --> convert to embedding dim\n",
    "        fc = [nn.Linear(image_input_dim, embedding_dim), nn.ReLU()]\n",
    "        fc.append(nn.Dropout(dropout))\n",
    "        self.wsi_net = nn.Sequential(*fc)\n",
    "\n",
    "        ## Text encoder -- CLIP\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if clip_model is None:\n",
    "            self.clip_model, _ = clip.load(\"ViT-B/32\", device=self.device, jit=False)\n",
    "        else: \n",
    "            self.clip_model = clip_model\n",
    "        self.clip_model.train()\n",
    "\n",
    "        # Text Linear Layer, map to embedding dim NOTE: 512 = clip.text_encoder output dim\n",
    "        txt_fc = [nn.Linear(512, embedding_dim), nn.ReLU()]\n",
    "        txt_fc.append(nn.Dropout(dropout))\n",
    "        self.text_net = nn.Sequential(*txt_fc) # for initial text features going into Co-Attn\n",
    "        # text logits\n",
    "        txt_logits_fc = [nn.Linear(512, logits_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
    "        self.text_logits_net = nn.Sequential(*txt_logits_fc) # for final text features before going into CLIP\n",
    "\n",
    "        ## Co-Attention\n",
    "        ### Multihead Attention\n",
    "        self.coattn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "        ### WSI Transformer + Attention Head\n",
    "        path_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dim_feedforward=logits_dim, dropout=dropout, activation='relu')\n",
    "        self.path_transformer = nn.TransformerEncoder(path_encoder_layer, num_layers=2)\n",
    "        self.path_attention_head = Attn_Net_Gated(L=logits_dim, D=logits_dim, dropout=dropout, n_classes=1)\n",
    "        self.path_rho = nn.Sequential(*[nn.Linear(logits_dim, embedding_dim), nn.ReLU(), nn.Dropout(dropout)]) # linear layer\n",
    "        \n",
    "    def encode_text(self, text: torch.Tensor): \n",
    "        \"\"\"\n",
    "        Modification of original CLIP encode_text that results in\n",
    "        embeddings for each token instead of taking the features from the\n",
    "        eot embedding. \n",
    "\n",
    "        See reference https://github.com/openai/CLIP/blob/main/clip/model.py\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.clip_model.token_embedding(text).type(self.clip_model.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.clip_model.positional_embedding.type(self.clip_model.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.clip_model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip_model.ln_final(x).type(self.clip_model.dtype)\n",
    "        return x\n",
    "\n",
    "    def forward(self, img: torch.Tensor, txt: torch.Tensor, verbose: bool = False): \n",
    "        \"\"\"\n",
    "        Given image and text, returns both image and text logits. Also returns\n",
    "        attention scores from co-attention, image, and text. \n",
    "\n",
    "        Args: \n",
    "            img: N x P x d_i x 1 x 1\n",
    "            txt: N x T x 1\n",
    "        Note: \n",
    "            batch_size is always equal to 1\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        ### Image linear layer\n",
    "        # print(\"img shape: \", img.shape)\n",
    "        # shape: (N x P x d_i)\n",
    "        img_bag = self.wsi_net(img)  ### path embeddings are fed through a FC layer\n",
    "        if len(img_bag.shape) == 2: \n",
    "            img_bag = img_bag.unsqueeze(1)\n",
    "        # print(\"img_bag shape: \", img_bag.shape)\n",
    "        # shape: (N x P x d_e)\n",
    "\n",
    "        ### Text encoder\n",
    "        # print(\"txt.shape: \", txt.shape)\n",
    "        text_bag_init = self.encode_text(txt) # obtain clip text encoder embeddings\n",
    "        text_bag_init_float = text_bag_init.float()\n",
    "        # shape: (N x T x d_t)\n",
    "        # print(\"text_bag.shape (before linear): \", text_bag_init_float.shape)\n",
    "        \n",
    "        # d_t --> d_e embedding dim to make same as img dim for coattn\n",
    "        text_bag = self.text_net(text_bag_init_float) \n",
    "        # shape: (N x T x d_e)\n",
    "        # print(\"text_bag.shape (after linear): \", text_bag.shape)\n",
    "        \n",
    "        ### Apply co-attn -- args: query, key, value\n",
    "        img_coattn, A_coattn = self.coattn(text_bag, img_bag, img_bag)\n",
    "        # print(\"img_coattn.shape: \", img_coattn.shape) # shape: (N x T x d_e)\n",
    "        # print(\"A_coattn.shape: \", A_coattn.shape) # shape: (P x T)\n",
    "        \n",
    "        ### Apply WSI transformer --> changes dim from embed_dim (d_e) to logits_dim (d_l)\n",
    "        img_trans = self.path_transformer(img_coattn) # shape: (N x T x d_l)\n",
    "        # print(\"img_trans.shape: \", img_trans.shape)\n",
    "\n",
    "        ## Apply Attention over the transformer features per token\n",
    "        A_img, img_features = self.path_attention_head(img_trans.squeeze(1)) # attention mechanism\n",
    "        # print(\"A_img.shape (before transpose): \", A_img.shape)\n",
    "        # A_img = torch.squeeze(A_img, 2) # remove last dim of 1 before softmax and matrix multiply\n",
    "        A_img = torch.transpose(A_img, 1, 2) # swap last two dims, 1 and token_length\n",
    "        # print(\"A_img.shape (after transpose): \", A_img.shape)\n",
    "        # print(\"img_features.shape: \", img_features.shape)\n",
    "        img_features = torch.bmm(F.softmax(A_img, dim=1), img_features) # (N x 1 x d_l)\n",
    "        img_features = torch.squeeze(img_features)\n",
    "        # print(\"img_features.shape (after mm): \", img_features.shape) # shape: (N x d_l)\n",
    "        img_features = self.path_rho(img_features) # shape: (N x d_l)\n",
    "\n",
    "        ### Normalize features\n",
    "        image_features = img_features / img_features.norm(dim=1, keepdim=True) # (N x d_l)\n",
    "        # aggregate tokens --> single text representation (see CLIP https://github.com/openai/CLIP/blob/main/clip/model.py#L354)\n",
    "        text_agg = text_bag_init[torch.arange(text_bag_init.shape[0]), txt.argmax(dim=-1)] @ self.clip_model.text_projection\n",
    "        text_agg = text_agg.float() # shape: (N x d_e)\n",
    "        # print(\"text_agg.shape: \", text_agg.shape, text_agg.dtype)\n",
    "        # convert from d_e --> d_l for text agg embeddings\n",
    "        text_agg = self.text_logits_net(text_agg) # shape: (N x d_l)\n",
    "        # linear layer to get into logits shape\n",
    "        text_features = text_agg / text_agg.norm(dim=1, keepdim=True) # shape: (N x d_l)\n",
    "        # print(\"image_features.shape: \", image_features.shape)\n",
    "        # print(\"text_features.shape: \", text_features.shape)\n",
    "\n",
    "        # image_features = torch.squeeze(image_features)\n",
    "        # text_features = torch.squeeze(text_features)\n",
    "        # print(\"image_features.shape (after squeeze): \", image_features.shape)\n",
    "        # print(\"text_features.shape (after squeeze): \", text_features.shape)\n",
    "        \n",
    "        ### Obtain dot product logits\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.clip_model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t() # shape: (N x N)\n",
    "        logits_per_text = logits_per_image.t() # shape: (N x N)\n",
    "\n",
    "        # print(\"logits_per_image.shape: \", logits_per_image.shape)\n",
    "        # print(\"logits_per_text.shape: \", logits_per_text.shape)\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        attention_scores = {'coattn': A_coattn, 'img': A_img}\n",
    "        return logits_per_image, logits_per_text, attention_scores\n",
    "\n",
    "class Attn_Net_Gated(nn.Module):\n",
    "    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n",
    "        r\"\"\"\n",
    "        See reference https://github.com/mahmoodlab/MCAT/blob/master/models/model_utils.py\n",
    "        Attention Network with Sigmoid Gating (3 fc layers)\n",
    "        args:\n",
    "            L (int): input feature dimension\n",
    "            D (int): hidden layer dimension\n",
    "            dropout (bool): whether to apply dropout (p = 0.25)\n",
    "            n_classes (int): number of classes\n",
    "        \"\"\"\n",
    "        super(Attn_Net_Gated, self).__init__()\n",
    "        self.attention_a = [\n",
    "            nn.Linear(L, D),\n",
    "            nn.Tanh()]\n",
    "        \n",
    "        self.attention_b = [nn.Linear(L, D), nn.Sigmoid()]\n",
    "        if dropout:\n",
    "            self.attention_a.append(nn.Dropout(0.25))\n",
    "            self.attention_b.append(nn.Dropout(0.25))\n",
    "\n",
    "        self.attention_a = nn.Sequential(*self.attention_a)\n",
    "        self.attention_b = nn.Sequential(*self.attention_b)\n",
    "        self.attention_c = nn.Linear(D, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.attention_a(x)\n",
    "        b = self.attention_b(x)\n",
    "        A = a.mul(b)\n",
    "        A = self.attention_c(A)  # N x n_classes\n",
    "        return A, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with PathZero model\n",
    "model = PathZero(clip_model=clip_model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 153316866 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The model has {num_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts, model):\n",
    "    _tokenizer = SimpleTokenizer()\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), model.context_length, dtype=torch.long)\n",
    "    \n",
    "    # will trim tokens to match context length\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > model.context_length:\n",
    "            tokens = tokens[:model.context_length]\n",
    "            tokens[model.context_length - 1] = eot_token\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30.1023, 21.9178, 22.7231, 27.2820, 22.4517, 24.2126, 30.1182, 24.7481],\n",
      "        [23.9379, 25.9622, 24.1611, 21.9616, 18.0911, 22.6735, 25.9819, 24.1141],\n",
      "        [29.6417, 31.0994, 26.9708, 27.7571, 20.5740, 27.0729, 31.9664, 25.8329],\n",
      "        [25.3947, 19.7767, 16.5627, 22.5389, 18.4853, 20.5879, 25.6718, 19.0675],\n",
      "        [21.9395, 27.3583, 21.4484, 22.0770, 17.7129, 18.1122, 21.6196, 18.3286],\n",
      "        [30.0496, 24.8225, 24.5744, 28.0990, 19.1271, 25.2953, 31.1733, 22.8713],\n",
      "        [23.7202, 23.9463, 19.4798, 22.0033, 14.2205, 16.1057, 25.7246, 18.9781],\n",
      "        [23.9488, 23.6184, 17.0340, 20.5861, 19.3954, 16.2704, 27.3494, 22.9458]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor([[30.1023, 23.9379, 29.6417, 25.3947, 21.9395, 30.0496, 23.7202, 23.9488],\n",
      "        [21.9178, 25.9622, 31.0994, 19.7767, 27.3583, 24.8225, 23.9463, 23.6184],\n",
      "        [22.7231, 24.1611, 26.9708, 16.5627, 21.4484, 24.5744, 19.4798, 17.0340],\n",
      "        [27.2820, 21.9616, 27.7571, 22.5389, 22.0770, 28.0990, 22.0033, 20.5861],\n",
      "        [22.4517, 18.0911, 20.5740, 18.4853, 17.7129, 19.1271, 14.2205, 19.3954],\n",
      "        [24.2126, 22.6735, 27.0729, 20.5879, 18.1122, 25.2953, 16.1057, 16.2704],\n",
      "        [30.1182, 25.9819, 31.9664, 25.6718, 21.6196, 31.1733, 25.7246, 27.3494],\n",
      "        [24.7481, 24.1141, 25.8329, 19.0675, 18.3286, 22.8713, 18.9781, 22.9458]],\n",
      "       device='cuda:0', grad_fn=<TBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for image, text, _ in data_loader:\n",
    "    text = preprocess_text(text, model.clip_model)\n",
    "    image = image.to(device)\n",
    "    text = text.to(device)\n",
    "    \n",
    "    image_logits, text_logits, attention_scores = model(image, text)\n",
    "\n",
    "    # training\n",
    "    print(image_logits)\n",
    "    print(text_logits)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr: float = 1e-4\n",
    "momentum: float = 0.9\n",
    "epochs: int = 4\n",
    "log_interval: int = 10\n",
    "save_interval: int = 150\n",
    "save_dir: Path = Path(\"./checkpoints\")\n",
    "model_name: str = \"path_zero_v0\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# init wandb config\n",
    "config = {\n",
    "    \"lr\": lr,\n",
    "    \"momentum\": momentum, \n",
    "    \"epochs\": epochs, \n",
    "    \"log_interval\": log_interval, \n",
    "    \"save_interval\": save_interval, \n",
    "    \"save_dir\": \"save_dir\", \n",
    "    \"model_name\": \"sample_0\", \n",
    "}\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"path-zero\",\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config=config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, device, criterion, optimizer, config): \n",
    "    model_save_dir = save_dir / model_name\n",
    "    model_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Run training\n",
    "    total_batches = len(loader) * epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    report_freq = log_interval\n",
    "    highest_val_auc = 0 # save highest mean auc\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0 # running loss over batch\n",
    "        for data in tqdm(loader):\n",
    "            # get the images\n",
    "            images, texts, _ = data\n",
    "            texts = preprocess_text(texts, model.clip_model) \n",
    "            \n",
    "            # perform step for a single batch\n",
    "            loss = train_batch(images, texts, model, device, criterion, optimizer)\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Report metrics every `report_freq` batch\n",
    "            if (batch_ct % report_freq) == 0:\n",
    "                train_log(running_loss / report_freq, example_ct, epoch)\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            if (batch_ct % save_interval) == 0: \n",
    "                model_path = model_save_dir / f\"checkpoint_{str(batch_ct)}.pt\"\n",
    "                print(\"Saved checkpoint to: \", model_path)\n",
    "                save(model, model_path)\n",
    "                \n",
    "def train_batch(images, texts, model, device, criterion, optimizer):\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "    \n",
    "    # Forward pass ➡\n",
    "    logits_per_image, logits_per_text, attention_scores = model(images, texts)\n",
    "\n",
    "    # Create labels\n",
    "    batch_size = images.shape[0]\n",
    "    labels = torch.arange(batch_size).to(device)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_img = criterion(logits_per_image, labels)\n",
    "    loss_txt = criterion(logits_per_text, labels)\n",
    "    loss = (loss_img + loss_txt)/2 # avg. img and txt loss\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # save to wandb \n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "    # print to log\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "def save(model, path): \n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 10/167 [00:06<01:44,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00080 examples: 3.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 20/167 [00:17<01:35,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00160 examples: 2.304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 30/167 [00:30<03:33,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00240 examples: 2.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 40/167 [00:43<04:01,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00320 examples: 2.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 50/167 [00:58<02:09,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00400 examples: 2.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 60/167 [01:09<02:17,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00480 examples: 2.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 70/167 [01:26<02:16,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00560 examples: 2.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 80/167 [01:36<01:25,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00640 examples: 2.090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 90/167 [01:49<01:17,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00720 examples: 2.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 100/167 [02:00<01:06,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00800 examples: 2.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 110/167 [02:07<00:45,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00880 examples: 2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 120/167 [02:20<01:10,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00960 examples: 2.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 130/167 [02:33<00:39,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01040 examples: 2.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 140/167 [02:43<00:24,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01120 examples: 2.084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 149/167 [02:54<00:22,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01200 examples: 2.084\n",
      "Saved checkpoint to:  checkpoints/path_zero_v0/checkpoint_150.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 160/167 [03:07<00:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01280 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:17<00:00,  1.18s/it]\n",
      "  2%|▏         | 3/167 [00:01<01:36,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01356 examples: 0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 13/167 [00:17<06:01,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01436 examples: 2.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 23/167 [00:25<02:29,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01516 examples: 2.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 33/167 [00:39<03:13,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01596 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 43/167 [00:55<04:17,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01676 examples: 2.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 53/167 [01:05<02:10,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01756 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 63/167 [01:17<02:25,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01836 examples: 2.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 73/167 [01:32<02:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01916 examples: 2.084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 83/167 [01:41<01:14,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 01996 examples: 2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 93/167 [01:54<01:05,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02076 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 103/167 [02:03<00:37,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02156 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 113/167 [02:13<01:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02236 examples: 2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 123/167 [02:27<01:17,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02316 examples: 2.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 132/167 [02:37<00:36,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02396 examples: 2.083\n",
      "Saved checkpoint to:  checkpoints/path_zero_v0/checkpoint_300.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 143/167 [02:47<00:20,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02476 examples: 2.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 153/167 [03:02<00:20,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02556 examples: 2.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 163/167 [03:13<00:07,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02636 examples: 2.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:17<00:00,  1.19s/it]\n",
      "  4%|▎         | 6/167 [00:03<01:50,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02712 examples: 1.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 16/167 [00:19<03:19,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02792 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 26/167 [00:28<02:35,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02872 examples: 2.083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 36/167 [00:39<01:41,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 02952 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 46/167 [00:57<02:58,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03032 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 56/167 [01:08<02:07,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03112 examples: 2.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 66/167 [01:23<03:15,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03192 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 76/167 [01:35<01:44,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03272 examples: 2.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 86/167 [01:48<01:56,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03352 examples: 2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 96/167 [01:57<01:10,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03432 examples: 2.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 106/167 [02:05<00:44,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03512 examples: 2.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 115/167 [02:13<00:51,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03592 examples: 2.081\n",
      "Saved checkpoint to:  checkpoints/path_zero_v0/checkpoint_450.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 126/167 [02:24<00:34,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03672 examples: 2.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 136/167 [02:33<00:28,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03752 examples: 2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 146/167 [02:44<00:32,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03832 examples: 2.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 156/167 [02:56<00:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03912 examples: 2.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 166/167 [03:10<00:01,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03992 examples: 2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [03:10<00:00,  1.14s/it]\n",
      "  5%|▌         | 9/167 [00:07<02:32,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04068 examples: 1.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 19/167 [00:20<02:05,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04148 examples: 2.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 29/167 [00:31<03:12,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04228 examples: 2.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 39/167 [00:43<02:48,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04308 examples: 2.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 49/167 [01:00<02:23,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04388 examples: 2.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 59/167 [01:09<01:26,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04468 examples: 2.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 69/167 [01:27<02:30,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04548 examples: 2.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 79/167 [01:38<01:51,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04628 examples: 2.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 89/167 [01:51<01:23,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04708 examples: 2.082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 98/167 [02:00<01:27,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04788 examples: 2.081\n",
      "Saved checkpoint to:  checkpoints/path_zero_v0/checkpoint_600.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 109/167 [02:08<00:51,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04868 examples: 2.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 119/167 [02:20<01:04,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 04948 examples: 2.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 126/167 [02:31<00:49,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[178], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, device, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     13\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;66;03m# running loss over batch\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(loader):\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# get the images\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         images, texts, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     17\u001b[0m         texts \u001b[38;5;241m=\u001b[39m preprocess_text(texts, model\u001b[38;5;241m.\u001b[39mclip_model) \n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py?line=1193'>1194</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py?line=1194'>1195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py?line=1195'>1196</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py?line=1196'>1197</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/std.py?line=1197'>1198</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=624'>625</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=625'>626</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=626'>627</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=627'>628</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=628'>629</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=629'>630</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=630'>631</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=631'>632</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=668'>669</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=669'>670</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=670'>671</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=671'>672</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=672'>673</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=55'>56</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=56'>57</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=57'>58</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=58'>59</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=59'>60</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=55'>56</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=56'>57</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=57'>58</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=58'>59</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=59'>60</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/CS224n_Final_Project/loader.py:80\u001b[0m, in \u001b[0;36mPathologyPairDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='file:///~/projects/CS224n_Final_Project/loader.py?line=77'>78</a>\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[1;32m     <a href='file:///~/projects/CS224n_Final_Project/loader.py?line=78'>79</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///~/projects/CS224n_Final_Project/loader.py?line=79'>80</a>\u001b[0m     img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(img_path)\n\u001b[1;32m     <a href='file:///~/projects/CS224n_Final_Project/loader.py?line=80'>81</a>\u001b[0m     img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(img)\n\u001b[1;32m     <a href='file:///~/projects/CS224n_Final_Project/loader.py?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=786'>787</a>\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=787'>788</a>\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=788'>789</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=789'>790</a>\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=790'>791</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1128'>1129</a>\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1129'>1130</a>\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1130'>1131</a>\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1132'>1133</a>\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1134'>1135</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1098'>1099</a>\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1099'>1100</a>\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1100'>1101</a>\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1102'>1103</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:1079\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1075'>1076</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1076'>1077</a>\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1078'>1079</a>\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1079'>1080</a>\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1080'>1081</a>\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1081'>1082</a>\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1082'>1083</a>\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py?line=1083'>1084</a>\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, data_loader, device, criterion, optimizer, config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/projects/CS224n_Final_Project/wandb/run-20230313_073643-xzuffw98</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/path-zero/path-clip-v0/runs/xzuffw98' target=\"_blank\">atomic-cloud-1</a></strong> to <a href='https://wandb.ai/path-zero/path-clip-v0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/path-zero/path-clip-v0' target=\"_blank\">https://wandb.ai/path-zero/path-clip-v0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/path-zero/path-clip-v0/runs/xzuffw98' target=\"_blank\">https://wandb.ai/path-zero/path-clip-v0/runs/xzuffw98</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/path-zero/path-clip-v0/runs/xzuffw98?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4c525e1970>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr: float = 1e-4\n",
    "momentum: float = 0.9\n",
    "epochs: int = 4\n",
    "log_interval: int = 100\n",
    "save_interval: int = 1000\n",
    "save_dir: Path = Path(\"./checkpoints\")\n",
    "model_name: str = \"sample_0\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# init wandb config\n",
    "config = {\n",
    "    \"lr\": lr,\n",
    "    \"momentum\": momentum, \n",
    "    \"epochs\": epochs, \n",
    "    \"log_interval\": log_interval, \n",
    "    \"save_interval\": save_interval, \n",
    "    \"save_dir\": \"save_dir\", \n",
    "    \"model_name\": \"sample_0\", \n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"path-clip-v0\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, device, criterion, optimizer, config): \n",
    "    model_save_dir = save_dir / model_name\n",
    "    model_save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Run training\n",
    "    total_batches = len(loader) * epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    report_freq = log_interval\n",
    "    highest_val_auc = 0 # save highest mean auc\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0 # running loss over batch\n",
    "        for data in tqdm(loader):\n",
    "            # get the images\n",
    "            images, texts, _ = data\n",
    "            texts = preprocess_text(texts, model) \n",
    "            \n",
    "            # perform step for a single batch\n",
    "            loss = train_batch(images, texts, model, device, criterion, optimizer)\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Report metrics every `report_freq` batch\n",
    "            if (batch_ct % report_freq) == 0:\n",
    "                train_log(running_loss / report_freq, example_ct, epoch)\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            if (batch_ct % save_interval) == 0: \n",
    "                model_path = model_save_dir / f\"checkpoint_{str(batch_ct)}.pt\"\n",
    "                print(\"Saved checkpoint to: \", model_path)\n",
    "                save(model, model_path)\n",
    "                \n",
    "def train_batch(images, texts, model, device, criterion, optimizer):\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "    \n",
    "    # Forward pass ➡\n",
    "    logits_per_image, logits_per_text = model(images, texts)\n",
    "    print(\"logits_per_image.shape: \", logits_per_image.shape)\n",
    "    print(\"logits_per_text.shape: \", logits_per_text.shape)\n",
    "    \n",
    "    # Create labels\n",
    "    batch_size = images.shape[0]\n",
    "    labels = torch.arange(batch_size).to(device)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_img = criterion(logits_per_image, labels)\n",
    "    loss_txt = criterion(logits_per_text, labels)\n",
    "    loss = (loss_img + loss_txt)/2 # avg. img and txt loss\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # save to wandb \n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "    # print to log\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "def save(model, path): \n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1332 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [768, 3, 32, 32], expected input[1, 1, 178, 2048] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[64], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, device, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     17\u001b[0m texts \u001b[38;5;241m=\u001b[39m preprocess_text(texts, model) \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# perform step for a single batch\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m example_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[1;32m     22\u001b[0m batch_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[64], line 39\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(images, texts, model, device, criterion, optimizer)\u001b[0m\n\u001b[1;32m     36\u001b[0m images, texts \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), texts\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass ➡\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m logits_per_image, logits_per_text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_per_image.shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, logits_per_image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_per_text.shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, logits_per_text\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1189'>1190</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1190'>1191</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1191'>1192</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1192'>1193</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1193'>1194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1194'>1195</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1195'>1196</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py:359\u001b[0m, in \u001b[0;36mCLIP.forward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=357'>358</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image, text):\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=358'>359</a>\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_image(image)\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=359'>360</a>\u001b[0m     text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_text(text)\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=361'>362</a>\u001b[0m     \u001b[39m# normalized features\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=339'>340</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=340'>341</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image\u001b[39m.\u001b[39;49mtype(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1189'>1190</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1190'>1191</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1191'>1192</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1192'>1193</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1193'>1194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1194'>1195</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1195'>1196</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py:224\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=222'>223</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=223'>224</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)  \u001b[39m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=224'>225</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# shape = [*, width, grid ** 2]\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/clip/model.py?line=225'>226</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# shape = [*, grid ** 2, width]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1189'>1190</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1190'>1191</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1191'>1192</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1192'>1193</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1193'>1194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1194'>1195</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1195'>1196</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=461'>462</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=462'>463</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=454'>455</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=455'>456</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=456'>457</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=457'>458</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=458'>459</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=459'>460</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 3, 32, 32], expected input[1, 1, 178, 2048] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "train(clip_model, data_loader, device, criterion, optimizer, config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
